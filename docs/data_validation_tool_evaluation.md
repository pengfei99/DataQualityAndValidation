# Data validation tool evaluation

## Great expectations

### Pros:
- Declarative validation rule 
- Rich built-in validation rule
- Easy to implement new validation rules
- Able to reuse existing validation rule set
- Provide a profiler to generate possible validation rules for a given dataset.
- Detailed validation report that indicates which validation rule failed and which row failed the validation rule
- Light language dependencies (validation rule in json, project config (e.g. checkpoint, data source) in yaml. Python only for running project)


### Cons:
- Validation rules generated by Profiler have low relevance.
- Many concept to understand

## TDDA

### Pros:
- Easy to install and use
- Declarative validation rule
- Able to reuse existing validation rule set
- Provide a profiler to generate possible validation rules for a given dataset.
- Validation rules generated by Profiler have high relevance.

### Cons:
- Can only take pandas dataframe as input
- Limit built-in validation rule
- Hard to implement new validation rules
- Validation report only indicates which validation rule failed
- Heavy python language dependencies. Although the validation rules(constrain) is in json. But all other operations all requires python.


## Bulwark

### Pros:
- Easy to install and use
- Declarative validation rule (with annotation).

### Cons:
- Can only take pandas dataframe as input
- Limit built-in validation rule
- Can't reuse existing validation rule set
- Hard to implement new validation rules
- No profiler to generate validation rules .
- No Validation report only boolean or exception to indicate rule failed
- Heavy python language dependencies. All operations require python, validation rules are function annotation or python function.


## Voluptuous

### Pros:
- Easy to install and use
- Declarative validation rule (custom python object contains a dictionary of rules).
- Can reuse existing validation rule set

### Cons:
- Can only take python dictionary (will lead to performance issue when df is bigger) as input
- Limit built-in validation rule
- Hard to implement new validation rules
- No profiler to generate validation rules .
- No Validation report only exception to indicate rule failed
- Heavy python language dependencies. All operations require python, validation rules are python object.
- performance issues when data set is bigger (>100Mo)
- Main contributor abandon the project, no more update or bug fixing.

## Deequ

### Pros:
- Provide a rich data profiling feature
- Declarative validation rule 
- Rich built-in validation rule
- Easy to implement new validation rules
- Can handle large data volume and valid it quickly


### Cons:
- Hard to reuse existing validation rule set
- No profiler to generate possible validation rules for a given dataset.
- Validation report is in form of spark data frame. Hard to export and read. 
- Heavy language dependencies (scala, python)
- Python API of Deequ has many bugs

## DBT

### Pros:
- easy to put in place/run
- can be integrated easily into each step of data transformation 
- Declarative validation rule 

### Cons:
- Only works with structure data (that can be read by dbt)
- No profiler for automatic data validation rule generation
- limit built in validation rules(only 4)
- custom validation rules need to be written in sql.
- can only validate at column level, can't check table level
- High language dependencies sql

## TensorFlow Data Validation

### Pros:
- Provide a basic data profiling feature
- Profiler can compare two data set side by side
- Generate validation rules for a given dataset.
- Able to reuse existing validation rule set (import schema)
- Detailed validation report (anomalies detection) that indicates which validation rule failed and which row failed the validation rule
- Detected anomalies have high relevance.
- Good integration with other tools in GCP


### Cons:
- Hard to install
- Can't customize validation rule, you can only relax the threshold of the validation rule (e.g. skew_comparator.infinity_norm.threshold = 0.01 or drift_comparator.infinity_norm.threshold = 0.001)
- High language dependencies, must master python