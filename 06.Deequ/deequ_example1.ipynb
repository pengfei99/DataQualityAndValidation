{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deequ\n",
    "\n",
    "## What is Deequ?\n",
    "In this example, we will use PyDeequ, an open-source Python wrapper over Deequ (an open-source tool developed and used at Amazon) to check data validity. Deequ is written in Scala, whereas PyDeequ allows you to use its data quality and testing capabilities from Python and PySpark. Furthermore, PyDeequ allows for fluid interface with Pandas DataFrames as opposed to restricting within Apache Spark DataFrames.\n",
    "\n",
    "**Deequ allows you to calculate data quality metrics on your dataset, define and verify data quality constraints, and be informed about changes in the data distribution**. Instead of implementing checks and verification algorithms on your own, you can focus on describing how your data should look. Deequ supports you by suggesting checks for you. Deequ is implemented on top of Apache Spark and is designed to scale with large datasets (billions of rows) that typically live in a data lake, distributed file system, or a data warehouse. PyDeequ gives you access to this capability, but also allows you to use it from the familiar environment of your Python Jupyter notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What are PyDeequâ€™s main components?\n",
    "\n",
    "- **Metrics computation** : Deequ computes data quality metrics, that is, statistics such as completeness, maximum, or correlation. Deequ uses Spark to read from sources and compute metrics through an optimized set of aggregation queries. You have direct access to the raw metrics computed on the data.\n",
    "- **Constraint verification** : With a given set of data quality constraints, Deequ generates a data quality report, which contains the result of the constraint verification.\n",
    "- **Constraint suggestion** : Deequ can generate constraint by profiling the data.\n",
    "- **Python wrappers** : You can call each Deequ function using Python syntax. The wrappers translate the commands to the underlying Deequ calls and return their response.\n",
    "\n",
    "\n",
    "Below diagram shows the PyDeequ's main component:\n",
    "![PyDeequ_architecture](../images/pydeequ_architecture.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please set env variable SPARK_VERSION\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, IntegerType,StringType\n",
    "from pydeequ.analyzers import *\n",
    "import pydeequ\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/26 11:01:07 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.184.146 instead (on interface ens33)\n",
      "22/01/26 11:01:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/spark-3.2.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pliu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pliu/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d5a2ff8c-2249-41af-8c5b-f9d3c09755fa;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "downloading https://repo1.maven.org/maven2/com/amazon/deequ/deequ/1.2.2-spark-3.0/deequ-1.2.2-spark-3.0.jar ...\n",
      "\t[SUCCESSFUL ] com.amazon.deequ#deequ;1.2.2-spark-3.0!deequ.jar (189ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.12/0.13.2/breeze_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze_2.12;0.13.2!breeze_2.12.jar (2186ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.12/0.13.2/breeze-macros_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze-macros_2.12;0.13.2!breeze-macros_2.12.jar (22ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (11ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (95ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.2!commons-math3.jar (177ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire_2.12/0.13.0/spire_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire_2.12;0.13.0!spire_2.12.jar (1040ms)\n",
      "downloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.2/shapeless_2.12-2.3.2.jar ...\n",
      "\t[SUCCESSFUL ] com.chuusai#shapeless_2.12;2.3.2!shapeless_2.12.jar(bundle) (316ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.5!slf4j-api.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.1/scala-reflect-2.12.1.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.1!scala-reflect.jar (356ms)\n",
      "downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] junit#junit;4.8.2!junit.jar (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.12/0.13.0/spire-macros_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire-macros_2.12;0.13.0!spire-macros_2.12.jar (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.12/0.6.1/machinist_2.12-0.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#machinist_2.12;0.6.1!machinist_2.12.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#macro-compat_2.12;1.1.1!macro-compat_2.12.jar (12ms)\n",
      ":: resolution report :: resolve 7064ms :: artifacts dl 4527ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   15  |   15  |   1   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d5a2ff8c-2249-41af-8c5b-f9d3c09755fa\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (32953kB/36ms)\n",
      "22/01/26 11:01:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=True\n",
    "deequ_jar_path=\"../lib/deequ-2.0.0-spark-3.1.jar\"\n",
    "if local:\n",
    "    spark=SparkSession.builder\\\n",
    "            .master(\"local[4]\")\\\n",
    "            .appName(\"deequ_example\") \\\n",
    "            .config(\"spark.driver.extraClassPath\", deequ_jar_path) \\\n",
    "            .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord) \\\n",
    "            .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord) \\\n",
    "            .getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder\\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\")\\\n",
    "        .appName(\"deequ_example\") \\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\") \\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\",\"8g\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", deequ_jar_path) \\\n",
    "        .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord) \\\n",
    "        .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord) \\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# read data\n",
    "val_file_path=\"../data/adult.csv\"\n",
    "test_file_path=\"../data/adult_with_duplicates.csv\"\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"age\",IntegerType(),True) \\\n",
    "      .add(\"workclass\",StringType(),True) \\\n",
    "      .add(\"fnlwgt\",IntegerType(),True) \\\n",
    "      .add(\"education\",StringType(),True) \\\n",
    "      .add(\"education-num\",IntegerType(),True) \\\n",
    "      .add(\"marital-status\",StringType(),True) \\\n",
    "      .add(\"occupation\",StringType(),True) \\\n",
    "      .add(\"relationship\",StringType(),True) \\\n",
    "      .add(\"race\",StringType(),True) \\\n",
    "      .add(\"sex\",StringType(),True) \\\n",
    "      .add(\"capital-gain\",IntegerType(),True) \\\n",
    "      .add(\"capital-loss\",IntegerType(),True) \\\n",
    "      .add(\"hours-per-week\",IntegerType(),True) \\\n",
    "      .add(\"native-country\",StringType(),True) \\\n",
    "      .add(\"income\",StringType(),True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|age|       workclass|fnlwgt|education|education-num|    marital-status|       occupation| relationship| race|   sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "| 39|       State-gov| 77516|Bachelors|           13|     Never-married|     Adm-clerical|Not-in-family|White|  Male|        2174|           0|            40| United-States| <=50K|\n",
      "| 50|Self-emp-not-inc| 83311|Bachelors|           13|Married-civ-spouse|  Exec-managerial|      Husband|White|  Male|           0|           0|            13| United-States| <=50K|\n",
      "| 38|         Private|215646|  HS-grad|            9|          Divorced|Handlers-cleaners|Not-in-family|White|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 53|         Private|234721|     11th|            7|Married-civ-spouse|Handlers-cleaners|      Husband|Black|  Male|           0|           0|            40| United-States| <=50K|\n",
      "| 28|         Private|338409|Bachelors|           13|Married-civ-spouse|   Prof-specialty|         Wife|Black|Female|           0|           0|            40|          Cuba| <=50K|\n",
      "+---+----------------+------+---------+-------------+------------------+-----------------+-------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_val=spark.read.option(\"header\", False).schema(schema).csv(val_file_path)\n",
    "df_val.show(5)\n",
    "df_val.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+------+---------+-------------+--------------+------------+-------------+-----+----+------------+------------+--------------+--------------+------+\n",
      "| age|     workclass|fnlwgt|education|education-num|marital-status|  occupation| relationship| race| sex|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+----+--------------+------+---------+-------------+--------------+------------+-------------+-----+----+------------+------------+--------------+--------------+------+\n",
      "| 139|     State-gov| 77516|Bachelors|           13| Never-married|Adm-clerical|Not-in-family|White|Male|        2174|           0|            40| United-States| <=50K|\n",
      "| -12|     State-gov| 77516|Bachelors|           13| Never-married|Adm-clerical|Not-in-family|White|Male|        2174|           0|            40| United-States| <=50K|\n",
      "|null|emp-by-pengfei| 77516|Bachelors|           13| Never-married|Adm-clerical|Not-in-family|White|Male|        2174|           0|            40| United-States| <=50K|\n",
      "|null|     State-gov| 77516|Bachelors|           13| Never-married|Adm-clerical|Not-in-family|White|Male|        2174|           0|            40| United-States| <=50K|\n",
      "|  39|     State-gov| 77516|Bachelors|           13| Never-married|Adm-clerical|Not-in-family|White|Male|        2174|           0|            40| United-States| <=50K|\n",
      "+----+--------------+------+---------+-------------+--------------+------------+-------------+-----+----+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- education-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test=spark.read.options(header='True', delimiter=',').schema(schema).csv(test_file_path)\n",
    "df_test.show(5)\n",
    "df_test.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data analysis\n",
    "\n",
    "Before we define checks on the data, we want to calculate some statistics on the dataset; we call this step data profiling. As with Deequ, PyDeequ supports a rich set of metrics. For more information, see [Test data quality at scale with Deequ](https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/) or the [GitHub repo](https://github.com/awslabs/deequ/tree/master/src/main/scala/com/amazon/deequ/analyzers). In the following example, we use the **AnalysisRunner** to capture the metrics on valid data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o89.run.\n: java.lang.AbstractMethodError: Receiver class org.apache.spark.sql.catalyst.expressions.aggregate.StatefulHyperloglogPlus does not define or inherit an implementation of the resolved method 'abstract org.apache.spark.sql.catalyst.trees.TreeNode withNewChildrenInternal(scala.collection.IndexedSeq)' of abstract class org.apache.spark.sql.catalyst.trees.TreeNode.\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$withNewChildren$2(TreeNode.scala:359)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:595)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:595)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:425)\n\tat org.apache.spark.sql.catalyst.util.package$.usePrettyExpression(package.scala:128)\n\tat org.apache.spark.sql.catalyst.util.package$.toPrettySQL(package.scala:158)\n\tat org.apache.spark.sql.RelationalGroupedDataset.alias(RelationalGroupedDataset.scala:89)\n\tat org.apache.spark.sql.RelationalGroupedDataset.$anonfun$toDF$1(RelationalGroupedDataset.scala:65)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:255)\n\tat org.apache.spark.sql.Dataset.agg(Dataset.scala:1890)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.liftedTree1$1(AnalysisRunner.scala:326)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.runScanningAnalyzers(AnalysisRunner.scala:318)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.doAnalysisRun(AnalysisRunner.scala:167)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunBuilder.run(AnalysisRunBuilder.scala:110)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m analysisResult \u001B[38;5;241m=\u001B[39m \u001B[43mAnalysisRunner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCompleteness\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mApproxCountDistinct\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMean\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCompliance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop age\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mage >= 30\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCorrelation\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmarital-status\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrelationship\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddAnalyzer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCorrelation\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrelationship\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                    \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/dataqualityandvalidation-6LYnP9NJ-py3.8/lib/python3.8/site-packages/pydeequ/analyzers.py:144\u001B[0m, in \u001B[0;36mAnalysisRunBuilder.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m    Run the Analysis.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \n\u001B[1;32m    142\u001B[0m \u001B[38;5;124;03m    :return: self: Runs the AnalysisRunBuilder.\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_AnalysisRunBuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/dataqualityandvalidation-6LYnP9NJ-py3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/dataqualityandvalidation-6LYnP9NJ-py3.8/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 111\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m py4j\u001B[38;5;241m.\u001B[39mprotocol\u001B[38;5;241m.\u001B[39mPy4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    113\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/dataqualityandvalidation-6LYnP9NJ-py3.8/lib/python3.8/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o89.run.\n: java.lang.AbstractMethodError: Receiver class org.apache.spark.sql.catalyst.expressions.aggregate.StatefulHyperloglogPlus does not define or inherit an implementation of the resolved method 'abstract org.apache.spark.sql.catalyst.trees.TreeNode withNewChildrenInternal(scala.collection.IndexedSeq)' of abstract class org.apache.spark.sql.catalyst.trees.TreeNode.\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$withNewChildren$2(TreeNode.scala:359)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:358)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:595)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:595)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:425)\n\tat org.apache.spark.sql.catalyst.util.package$.usePrettyExpression(package.scala:128)\n\tat org.apache.spark.sql.catalyst.util.package$.toPrettySQL(package.scala:158)\n\tat org.apache.spark.sql.RelationalGroupedDataset.alias(RelationalGroupedDataset.scala:89)\n\tat org.apache.spark.sql.RelationalGroupedDataset.$anonfun$toDF$1(RelationalGroupedDataset.scala:65)\n\tat scala.collection.immutable.List.map(List.scala:297)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:255)\n\tat org.apache.spark.sql.Dataset.agg(Dataset.scala:1890)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.liftedTree1$1(AnalysisRunner.scala:326)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.runScanningAnalyzers(AnalysisRunner.scala:318)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunner$.doAnalysisRun(AnalysisRunner.scala:167)\n\tat com.amazon.deequ.analyzers.runners.AnalysisRunBuilder.run(AnalysisRunBuilder.scala:110)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df_val) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"age\")) \\\n",
    "                    .addAnalyzer(ApproxCountDistinct(\"age\")) \\\n",
    "                    .addAnalyzer(Mean(\"age\")) \\\n",
    "                    .addAnalyzer(Compliance(\"top age\", \"age >= 30\")) \\\n",
    "                    .addAnalyzer(Correlation(\"marital-status\", \"relationship\")) \\\n",
    "                    .addAnalyzer(Correlation(\"relationship\", \"age\")) \\\n",
    "                    .run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}