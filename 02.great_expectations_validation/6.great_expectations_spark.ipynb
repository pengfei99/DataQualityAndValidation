{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e09f8e-c623-4273-a718-2e8cf63dbbba",
   "metadata": {},
   "source": [
    "# Great Expectations on Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcc101-824b-48d4-8e9f-9aef3e1e9f22",
   "metadata": {},
   "source": [
    "Implementing unit tests on Pyspark DataFrames using Great Expections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1050cbce-99e2-48ae-a778-b91625680644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07301707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 09:45:54 WARN Utils: Your hostname, pengfei-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 10.50.2.80 instead (on interface eth0)\n",
      "23/01/12 09:45:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/12 09:45:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Great Expectations with Pandas DataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801572a-7b3e-4f03-b6f9-035740e43bfa",
   "metadata": {},
   "source": [
    "## Load Raw DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad2811-d814-415c-afcd-4d5682bf4629",
   "metadata": {},
   "source": [
    "##### Load data from file and create a Pyspark Data Frame View named CAMPAIGNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0be69a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_df = spark.read.option(\"header\", True).option(\"inferSchema\",True).csv(\"../data/Kickstarter_projects_Feb19.csv\")\n",
    "raw_df.createOrReplaceTempView(\"CAMPAIGNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfd49ac-3e46-48fb-b0c3-2336e7def3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "pyspark.sql.dataframe.DataFrame"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_df is a spark dataframe\n",
    "type(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1de4187f-4207-4bc7-bd1d-2899ca7859a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- main_category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- launched_at: string (nullable = true)\n",
      " |-- deadline: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- goal_usd: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- blurb_length: string (nullable = true)\n",
      " |-- name_length: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- start_month: string (nullable = true)\n",
      " |-- end_month: string (nullable = true)\n",
      " |-- start_Q: string (nullable = true)\n",
      " |-- end_Q: string (nullable = true)\n",
      " |-- usd_pledged: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cd5303c-c94c-4bba-b7a2-c3d0d3d1d0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/dataqualityandvalidation-N5_6aXR_-py3.8/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# we can convert the spark dataframe to pandas dataframe\n",
    "raw_pdf=raw_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192548, 20)\n"
     ]
    }
   ],
   "source": [
    "print(raw_pdf.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "           id                                              name currency  \\\n0  1687733153             Socks of Speed and Socks of Elvenkind      USD   \n1   227936657  Power Punch Boot Camp: An All-Ages Graphic Novel      GBP   \n2   454186436    \"Live Printing with SX8: \"\"Squeegee Pulp Up\"\"\"      USD   \n3   629469071                 Lost Dog Street Band's Next Album      USD   \n4   183973060                             Qto-X, a Tiny Lantern      USD   \n\n  main_category    sub_category          launched_at             deadline  \\\n0         games  Tabletop Games  2018-10-30 20:00:02  2018-11-15 17:59:00   \n1        comics     Comic Books  2018-08-06 10:00:43  2018-09-05 10:00:43   \n2       fashion         Apparel  2017-06-09 15:41:03  2017-07-09 15:41:03   \n3         music  Country & Folk  2014-09-25 18:46:01  2014-11-10 06:00:00   \n4    technology         Gadgets  2016-11-28 16:35:11  2017-01-27 16:35:11   \n\n  duration    goal_usd        city    state country blurb_length name_length  \\\n0     16.0      2000.0     Menasha       WI      US           14           7   \n1     30.0  3870.99771  Shepperton  England      GB           24           8   \n2     30.0      1100.0   Manhattan       NY      US           21           7   \n3     45.0      3500.0   Nashville       TN      US           15           6   \n4     60.0     30000.0        Troy       MI      US           15           4   \n\n       status start_month end_month start_Q end_Q        usd_pledged  \n0  successful          10        11      Q4    Q4             6061.0  \n1  successful           8         9      Q3    Q3  3914.505120400001  \n2  successful           6         7      Q2    Q3             1110.0  \n3  successful           9        11      Q3    Q4             4807.0  \n4  successful          11         1      Q4    Q1            40368.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>currency</th>\n      <th>main_category</th>\n      <th>sub_category</th>\n      <th>launched_at</th>\n      <th>deadline</th>\n      <th>duration</th>\n      <th>goal_usd</th>\n      <th>city</th>\n      <th>state</th>\n      <th>country</th>\n      <th>blurb_length</th>\n      <th>name_length</th>\n      <th>status</th>\n      <th>start_month</th>\n      <th>end_month</th>\n      <th>start_Q</th>\n      <th>end_Q</th>\n      <th>usd_pledged</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1687733153</td>\n      <td>Socks of Speed and Socks of Elvenkind</td>\n      <td>USD</td>\n      <td>games</td>\n      <td>Tabletop Games</td>\n      <td>2018-10-30 20:00:02</td>\n      <td>2018-11-15 17:59:00</td>\n      <td>16.0</td>\n      <td>2000.0</td>\n      <td>Menasha</td>\n      <td>WI</td>\n      <td>US</td>\n      <td>14</td>\n      <td>7</td>\n      <td>successful</td>\n      <td>10</td>\n      <td>11</td>\n      <td>Q4</td>\n      <td>Q4</td>\n      <td>6061.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>227936657</td>\n      <td>Power Punch Boot Camp: An All-Ages Graphic Novel</td>\n      <td>GBP</td>\n      <td>comics</td>\n      <td>Comic Books</td>\n      <td>2018-08-06 10:00:43</td>\n      <td>2018-09-05 10:00:43</td>\n      <td>30.0</td>\n      <td>3870.99771</td>\n      <td>Shepperton</td>\n      <td>England</td>\n      <td>GB</td>\n      <td>24</td>\n      <td>8</td>\n      <td>successful</td>\n      <td>8</td>\n      <td>9</td>\n      <td>Q3</td>\n      <td>Q3</td>\n      <td>3914.505120400001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>454186436</td>\n      <td>\"Live Printing with SX8: \"\"Squeegee Pulp Up\"\"\"</td>\n      <td>USD</td>\n      <td>fashion</td>\n      <td>Apparel</td>\n      <td>2017-06-09 15:41:03</td>\n      <td>2017-07-09 15:41:03</td>\n      <td>30.0</td>\n      <td>1100.0</td>\n      <td>Manhattan</td>\n      <td>NY</td>\n      <td>US</td>\n      <td>21</td>\n      <td>7</td>\n      <td>successful</td>\n      <td>6</td>\n      <td>7</td>\n      <td>Q2</td>\n      <td>Q3</td>\n      <td>1110.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>629469071</td>\n      <td>Lost Dog Street Band's Next Album</td>\n      <td>USD</td>\n      <td>music</td>\n      <td>Country &amp; Folk</td>\n      <td>2014-09-25 18:46:01</td>\n      <td>2014-11-10 06:00:00</td>\n      <td>45.0</td>\n      <td>3500.0</td>\n      <td>Nashville</td>\n      <td>TN</td>\n      <td>US</td>\n      <td>15</td>\n      <td>6</td>\n      <td>successful</td>\n      <td>9</td>\n      <td>11</td>\n      <td>Q3</td>\n      <td>Q4</td>\n      <td>4807.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>183973060</td>\n      <td>Qto-X, a Tiny Lantern</td>\n      <td>USD</td>\n      <td>technology</td>\n      <td>Gadgets</td>\n      <td>2016-11-28 16:35:11</td>\n      <td>2017-01-27 16:35:11</td>\n      <td>60.0</td>\n      <td>30000.0</td>\n      <td>Troy</td>\n      <td>MI</td>\n      <td>US</td>\n      <td>15</td>\n      <td>4</td>\n      <td>successful</td>\n      <td>11</td>\n      <td>1</td>\n      <td>Q4</td>\n      <td>Q1</td>\n      <td>40368.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf.head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "c292e7c9-f672-433b-bd1c-8adcfed35be5",
   "metadata": {},
   "source": [
    "## Step1: Unit Tests on Raw Data\n",
    "\n",
    "To use GreatExpectation **expectations/validators** on dataframe, we need to first convert the `spark dataframe` to `ge SparkDFDataset`. You can find a full list of available `expectations` via this [page](https://greatexpectations.io/expectations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa4d98-1ec4-473b-b704-ad1ce6fc0766",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529d4bbd-d8f2-4c4f-90e4-2b8f9e0a3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_expectations.dataset import SparkDFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58dccdd1-faf2-475a-828d-369691c831b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before conversion <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "after conversion <class 'great_expectations.dataset.sparkdf_dataset.SparkDFDataset'>\n"
     ]
    }
   ],
   "source": [
    "# convert spark dataframe to ge sparkdf\n",
    "print(f\"before conversion {type(raw_df)}\")\n",
    "raw_test_df = SparkDFDataset(raw_df)\n",
    "print(f\"after conversion {type(raw_test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57e1dc-db7c-4698-ba3f-7c95e073a39d",
   "metadata": {},
   "source": [
    "### 1.1 Test 1: Check if mandatory columns exist\n",
    "\n",
    "Here we use a `expectation/validation` function called `expect_column_to_exist`. It takes a column name and returns a dictionary with various attributes. Below is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for : {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_to_exist\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"id\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {},\n",
      "  \"meta\": {},\n",
      "  \"success\": true,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test with a valid column name\n",
    "result=raw_test_df.expect_column_to_exist(\"id\")\n",
    "print(f\"result for : {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for : {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_to_exist\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"toto\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {},\n",
      "  \"meta\": {},\n",
      "  \"success\": false,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test with an invalid column name\n",
    "result=raw_test_df.expect_column_to_exist(\"toto\")\n",
    "print(f\"result for : {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can notice the first part of the dictionary describes the expectation (validation rule) and args. The second part is the result, in this example, there is nothing because this rule is very simple, no need to give extra information to understand the result. The most important attribute is **success** (bool), which tells us if the expectation is passed or not. In our example, it has value true when passes, otherwise it has value false. For more information, you can visit this [page](https://docs.greatexpectations.io/docs/terms/validation_result/)\n",
    "\n",
    "We can use this `expectation` to test if a dataframe contains a list of mandatory columns or not\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2f75de-701b-46ff-b93f-6487060285f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANDATORY_COLUMNS = [\n",
    "  \"id\",\n",
    "  \"currency\",\n",
    "  \"main_category\",\n",
    "  \"launched_at\",\n",
    "  \"deadline\",\n",
    "  \"country\",\n",
    "  \"status\",\n",
    "  \"usd_pledged\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffcc7029-53fc-44e3-b7a4-f3f64af48dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column id exists : PASSED\n",
      "Column currency exists : PASSED\n",
      "Column main_category exists : PASSED\n",
      "Column launched_at exists : PASSED\n",
      "Column deadline exists : PASSED\n",
      "Column country exists : PASSED\n",
      "Column status exists : PASSED\n",
      "Column usd_pledged exists : PASSED\n"
     ]
    }
   ],
   "source": [
    "for column in MANDATORY_COLUMNS:\n",
    "    try:\n",
    "        assert raw_test_df.expect_column_to_exist(column).success, f\"Uh oh! Mandatory column {column} does not exist: FAILED\"\n",
    "        print(f\"Column {column} exists : PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make a reusable, we can define a function\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def expect_df_to_contain_columns(df:SparkDFDataset,colList: List[str]):\n",
    "    badColList=[]\n",
    "    for column in colList:\n",
    "        if not df.expect_column_to_exist(column).success:\n",
    "            badColList.append(column)\n",
    "    if len(badColList)>0:\n",
    "        return False,badColList\n",
    "    else:\n",
    "        return True,badColList\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation status: True, output: []\n"
     ]
    }
   ],
   "source": [
    "status,output=expect_df_to_contain_columns(raw_test_df,\n",
    "                             MANDATORY_COLUMNS)\n",
    "\n",
    "print(f\"Expectation status: {status}, output: {output}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation status: False, output: ['toto', 'titi', 'tata']\n"
     ]
    }
   ],
   "source": [
    "status,output=expect_df_to_contain_columns(raw_test_df,[\"toto\",\"titi\",\"tata\"])\n",
    "\n",
    "print(f\"Expectation status: {status}, output: {output}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Test 2: Check if column value has specific type\n",
    "\n",
    "Here, we have two possible function which we can use:\n",
    "- expect_column_values_to_be_of_type(colName, type)\n",
    "- expect_column_values_to_be_in_type_list\n",
    "\n",
    "For more information, please visit this [page](https://greatexpectations.io/expectations/expect_column_values_to_be_of_type)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for : {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"id\",\n",
      "      \"type_\": \"IntegerType\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"observed_value\": \"IntegerType\"\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"success\": true,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "# test with a valid type\n",
    "result=raw_test_df.expect_column_values_to_be_of_type(\"id\",\"IntegerType\")\n",
    "print(f\"result content :\\n {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result for : {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"id\",\n",
      "      \"type_\": \"StringType\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"observed_value\": \"IntegerType\"\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"success\": false,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test with an invalid type\n",
    "result=raw_test_df.expect_column_values_to_be_of_type(\"id\",\"StringType\")\n",
    "print(f\"result for : {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the above example, we can check if the column of the dataframe has certain type. Note if the data source is semi-structural, the column type is probably inferred by spark. So you may need to do give a schema when you create the data frame.\n",
    "\n",
    "And this time the result has an attribute called **observed_value**, this value is from the dataframe current schema."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "1cd0f66e-2b2e-4165-bff9-a4d97db0060f",
   "metadata": {},
   "source": [
    "### 1.3 Test 3: Check if mandatory columns contains null rows\n",
    "\n",
    "We can use **expect_column_values_to_not_be_null**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result content :\n",
      " {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"id\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"element_count\": 192548,\n",
      "    \"unexpected_count\": 0,\n",
      "    \"unexpected_percent\": 0.0,\n",
      "    \"unexpected_percent_total\": 0.0,\n",
      "    \"partial_unexpected_list\": []\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"success\": true,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test with column id\n",
    "result=raw_test_df.expect_column_values_to_not_be_null(\"id\")\n",
    "print(f\"result content :\\n {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result content :\n",
      " {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"main_category\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"element_count\": 192548,\n",
      "    \"unexpected_count\": 1,\n",
      "    \"unexpected_percent\": 0.0005193510189666992,\n",
      "    \"unexpected_percent_total\": 0.0005193510189666992,\n",
      "    \"partial_unexpected_list\": [\n",
      "      null\n",
      "    ]\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"success\": false,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test with column main_category\n",
    "result=raw_test_df.expect_column_values_to_not_be_null(\"main_category\")\n",
    "print(f\"result content :\\n {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result contains much useful information:\n",
    "\n",
    "```text\n",
    "\"result\": {\n",
    "    \"element_count\": 192548, # total row\n",
    "    \"unexpected_count\": 1, # null row\n",
    "    \"unexpected_percent\": 0.0005193510189666992,\n",
    "    \"unexpected_percent_total\": 0.0005193510189666992,\n",
    "    \"partial_unexpected_list\": [\n",
    "      null\n",
    "    ]\n",
    "  }\n",
    "\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b845d31-9b6a-43e6-b3b1-962e0e825946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column id are not null: PASSED\n",
      "All items in column currency are not null: PASSED\n",
      "Uh oh! 1 of 192548 items in column main_category are null: FAILED\n",
      "Uh oh! 1 of 192548 items in column launched_at are null: FAILED\n",
      "Uh oh! 1 of 192548 items in column deadline are null: FAILED\n",
      "Uh oh! 1 of 192548 items in column country are null: FAILED\n",
      "Uh oh! 1 of 192548 items in column status are null: FAILED\n",
      "Uh oh! 1 of 192548 items in column usd_pledged are null: FAILED\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "for column in MANDATORY_COLUMNS:\n",
    "    try:\n",
    "        test_result = raw_test_df.expect_column_values_to_not_be_null(column)\n",
    "        assert test_result.success, \\\n",
    "            f\"Uh oh! {test_result.result['unexpected_count']} of {test_result.result['element_count']} items in column {column} are null: FAILED\"\n",
    "        print(f\"All items in column {column} are not null: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(e) \n",
    "    except AnalysisException as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126629d-c209-4e09-9bc0-33abdca509e5",
   "metadata": {},
   "source": [
    "### 1.4 Test 4: Check if launched_at column is a valid datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result content :\n",
      " {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_values_to_match_strftime_format\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"launched_at\",\n",
      "      \"strftime_format\": \"%Y-%m-%d %H:%M:%S\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"element_count\": 192548,\n",
      "    \"missing_count\": 1,\n",
      "    \"missing_percent\": 0.0005193510189666992,\n",
      "    \"unexpected_count\": 562,\n",
      "    \"unexpected_percent\": 0.2918767885243603,\n",
      "    \"unexpected_percent_total\": 0.2918752726592849,\n",
      "    \"unexpected_percent_nonmissing\": 0.2918767885243603,\n",
      "    \"partial_unexpected_list\": [\n",
      "      \"Hip-Hop\",\n",
      "      \"Rock\",\n",
      "      \"Webseries\",\n",
      "      \"Musical\",\n",
      "      \"Kids\",\n",
      "      \"film & video\",\n",
      "      \"Painting\",\n",
      "      \"Webseries\",\n",
      "      \"Comedy\",\n",
      "      \"World Music\",\n",
      "      \"Children's Books\",\n",
      "      \"Indie Rock\",\n",
      "      \"Documentary\",\n",
      "      \"Cookbooks\",\n",
      "      \"Country & Folk\",\n",
      "      \"Hip-Hop\",\n",
      "      \"Jazz\",\n",
      "      \"Shorts\",\n",
      "      \"Classical Music\",\n",
      "      \"Art Books\"\n",
      "    ]\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"success\": false,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# test with column id\n",
    "result=raw_test_df.expect_column_values_to_match_strftime_format('launched_at','%Y-%m-%d %H:%M:%S')\n",
    "print(f\"result content :\\n {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|        launched_at|count|\n",
      "+-------------------+-----+\n",
      "|2018-10-10 14:58:06|    3|\n",
      "|             People|    3|\n",
      "|2018-06-12 16:04:59|    3|\n",
      "|2017-11-15 23:29:31|    3|\n",
      "|               Kids|    3|\n",
      "|2018-11-26 17:30:30|    3|\n",
      "|2017-03-15 00:15:53|    3|\n",
      "|        Young Adult|    3|\n",
      "|2018-01-31 18:00:33|    3|\n",
      "|              Music|    3|\n",
      "|           Textiles|    3|\n",
      "|2014-11-21 18:01:56|    3|\n",
      "|              games|    3|\n",
      "|2018-10-02 11:05:11|    3|\n",
      "|2016-09-06 19:49:38|    3|\n",
      "|2018-03-27 15:01:06|    3|\n",
      "|2018-02-01 22:34:51|    3|\n",
      "|               food|    3|\n",
      "|2017-07-01 20:27:21|    3|\n",
      "|           Academic|    3|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "raw_df.groupby(col(\"launched_at\")).count().orderBy(col(\"count\")).filter(col(\"count\")>2).show(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With above example, we are sure someone messed with the column, the result section are very useful\n",
    "\n",
    "```text\n",
    "\"result\": {\n",
    "    \"element_count\": 192548, # all rows\n",
    "    \"missing_count\": 1,  # null rows\n",
    "    \"missing_percent\": 0.0005193510189666992,\n",
    "    \"unexpected_count\": 562, # row that does not match the format\n",
    "    \"unexpected_percent\": 0.2918767885243603,\n",
    "    \"unexpected_percent_total\": 0.2918752726592849,\n",
    "    \"unexpected_percent_nonmissing\": 0.2918767885243603,\n",
    "    \"partial_unexpected_list\": [  # row values that does not match\n",
    "      \"Hip-Hop\",\n",
    "      \"Rock\",\n",
    "      \"Webseries\",\n",
    "      \"Musical\",\n",
    "      \"Kids\",\n",
    "      \"film & video\",\n",
    "      ...]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a4ac1bf-ae26-45fd-a96e-e2cb654602a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "'0.29% is not a valid date time format'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result =  raw_test_df.expect_column_values_to_match_strftime_format('launched_at','%Y-%m-%d %H:%M:%S')\n",
    "f\"\"\"{round(test_result.result['unexpected_percent'], 2)}% is not a valid date time format\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12065115-26cc-4e9f-b2bb-61a37d7c5a24",
   "metadata": {},
   "source": [
    "### 1.5 Test 5: Check if deadline is a valid datetime format\n",
    "\n",
    "Same for dealine column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fdfc143-4453-4492-8fd8-7551b213e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "'0.04% is not a valid date time format'"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result =  raw_test_df.expect_column_values_to_match_strftime_format('deadline','%Y-%m-%d %H:%M:%S')\n",
    "f\"\"\"{round(test_result.result['unexpected_percent'], 2)}% is not a valid date time format\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c10ed6-2ecd-4e31-8cc9-f044a747569e",
   "metadata": {},
   "source": [
    "### 1.6 Test 6: Check if id is unique\n",
    "\n",
    "Some column values must be unique. We can use the function **expect_column_values_to_be_unique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result content :\n",
      " {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_values_to_be_unique\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"id\",\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"element_count\": 192548,\n",
      "    \"missing_count\": 0,\n",
      "    \"missing_percent\": 0.0,\n",
      "    \"unexpected_count\": 48162,\n",
      "    \"unexpected_percent\": 25.01298377547417,\n",
      "    \"unexpected_percent_total\": 25.01298377547417,\n",
      "    \"unexpected_percent_nonmissing\": 25.01298377547417,\n",
      "    \"partial_unexpected_list\": [\n",
      "      39036,\n",
      "      39036,\n",
      "      39235,\n",
      "      39235,\n",
      "      50419,\n",
      "      50419,\n",
      "      188790,\n",
      "      188790,\n",
      "      342881,\n",
      "      342881,\n",
      "      358771,\n",
      "      358771,\n",
      "      377517,\n",
      "      377517,\n",
      "      390870,\n",
      "      390870,\n",
      "      442565,\n",
      "      442565,\n",
      "      538372,\n",
      "      538372\n",
      "    ]\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"success\": false,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test with column id\n",
    "result=raw_test_df.expect_column_values_to_be_unique('id')\n",
    "print(f\"result content :\\n {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the above example, we can have the following output, we can detect all duplicated row values in column id\n",
    "\n",
    "```text\n",
    "'element_count': 192548,\n",
    "'missing_count': 0,\n",
    "'missing_percent': 0.0,\n",
    "'unexpected_count': 48162,\n",
    "'unexpected_percent': 25.01298377547417,\n",
    "'unexpected_percent_total': 25.01298377547417,\n",
    "'unexpected_percent_nonmissing': 25.01298377547417,\n",
    "'partial_unexpected_list': [39036, 39036, 39235, 39235, 50419, 50419, 188790, 188790, 342881, 342881, 358771, 358771, 377517, 377517, 390870, 390870, 442565, 442565, 538372, 538372]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'element_count': 192548, 'missing_count': 0, 'missing_percent': 0.0, 'unexpected_count': 48162, 'unexpected_percent': 25.01298377547417, 'unexpected_percent_total': 25.01298377547417, 'unexpected_percent_nonmissing': 25.01298377547417, 'partial_unexpected_list': [39036, 39036, 39235, 39235, 50419, 50419, 188790, 188790, 342881, 342881, 358771, 358771, 377517, 377517, 390870, 390870, 442565, 442565, 538372, 538372]}\n"
     ]
    }
   ],
   "source": [
    "print(result.result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfd9127e-df76-46e3-a99c-681e6112f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uh oh! 48162 of 192548 items or 25.01% are not unique: FAILED\n"
     ]
    }
   ],
   "source": [
    "test_result = raw_test_df.expect_column_values_to_be_unique(\"id\")\n",
    "failed_msg = \" \".join([f\"\"\"Uh oh!\"\"\",\n",
    "              f\"\"\"{test_result.result['unexpected_count']} of {test_result.result['element_count']} items\"\"\",\n",
    "              f\"\"\"or {round(test_result.result['unexpected_percent'],2)}% are not unique: FAILED\"\"\"])\n",
    "print(f\"\"\"{'Column id is unique: PASSED' if test_result.success else failed_msg}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc94c0-ea6e-4ced-9ac2-2fb4c5898aa1",
   "metadata": {},
   "source": [
    "## 2 Step2 : Filter Data\n",
    "\n",
    "### Business Rules\n",
    "\n",
    "1. Should filter campaigns in country \"US\" only\n",
    "2. Should filter campaigns that were active in 2017 and 2018 only based on \"launch_at\" and \"deadline\".\n",
    "3. Include only the below categories:\n",
    "    - art\n",
    "    - publishing\n",
    "    - film & video\n",
    "    - technology\n",
    "    - journalism\n",
    "    - food\n",
    "    - dance\n",
    "    - photography\n",
    "    - games\n",
    "    - crafts\n",
    "    - music\n",
    "    - comics\n",
    "    - theater\n",
    "    - design\n",
    "4. Should include successful campaigns only\n",
    "5. Should include USD currency only\n",
    "\n",
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7d6a240-a26d-4bae-b17f-dde9c0208386",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_CATEGORIES = [\n",
    "    'art',\n",
    "    'publishing',\n",
    "    'film & video',\n",
    "    'technology',\n",
    "    'journalism',\n",
    "    'food',\n",
    "    'dance',\n",
    "    'photography',\n",
    "    'games',\n",
    "    'crafts',\n",
    "    'music',\n",
    "    'comics',\n",
    "    'theater',\n",
    "    'design'    \n",
    "]\n",
    "ASSESSMENT_YEAR = ['2017','2018']\n",
    "COUNTRY = 'US'\n",
    "CURRENCY = 'USD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627614a-ec08-4c5a-a378-f4226baa0faf",
   "metadata": {},
   "source": [
    "#### Generate a Reference Data for Assessment Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4723e558-1a00-49f2-8d3d-8e74baee4303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  assessment_year period_start_dt period_end_dt\n0            2017      2016-07-01    2017-06-30\n1            2018      2017-07-01    2018-06-30",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>assessment_year</th>\n      <th>period_start_dt</th>\n      <th>period_end_dt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017</td>\n      <td>2016-07-01</td>\n      <td>2017-06-30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018</td>\n      <td>2017-07-01</td>\n      <td>2018-06-30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assessment_year_reference = {\n",
    "    'assessment_year': ['2017', '2018'], \n",
    "    'period_start_dt': ['2016-07-01', '2017-07-01'],\n",
    "    'period_end_dt': ['2017-06-30', '2018-06-30'],\n",
    "}\n",
    "ay_df = pd.DataFrame(data=assessment_year_reference)\n",
    "ay_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7bdcd-702e-4d07-99bc-c37794b42244",
   "metadata": {},
   "source": [
    "#### Convert to a Pyspark DataFrame to be able to join to CAMPAIGNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6052cf17-19b2-49db-a2df-2b9a4a6acfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/dataqualityandvalidation-N5_6aXR_-py3.8/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    },
    {
     "data": {
      "text/plain": "pyspark.sql.dataframe.DataFrame"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_ay_df = spark.createDataFrame(ay_df) \n",
    "spark_ay_df.createOrReplaceTempView(\"assessment_year_ref\")\n",
    "type(spark_ay_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b742b5-e495-4b1e-ba33-2377ffc50dfa",
   "metadata": {},
   "source": [
    "#### Apply Transformation and create a view named FILTERED_CAMPAIGNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0d7691c-1fae-4c04-b169-f853d92357c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = spark.sql(f\"\"\"\n",
    "    SELECT id,\n",
    "           name,\n",
    "           currency,\n",
    "           main_category,\n",
    "           launched_at,\n",
    "           deadline,\n",
    "           goal_usd,\n",
    "           country,\n",
    "           usd_pledged,\n",
    "           status,\n",
    "           assessment_year\n",
    "    FROM (SELECT t.*,\n",
    "               ay.assessment_year,\n",
    "               row_number() OVER (\n",
    "                   PARTITION BY t.id\n",
    "                   ORDER BY t.launched_at, \n",
    "                            ay.assessment_year DESC) row_no\n",
    "          FROM CAMPAIGNS t\n",
    "          INNER JOIN assessment_year_ref ay\n",
    "              ON TO_DATE(t.launched_at) <= ay.period_end_dt \n",
    "              AND t.deadline > ay.period_start_dt\n",
    "          WHERE country = '{COUNTRY}'\n",
    "          AND status = 'successful'\n",
    "          AND main_category IN ('{\"','\".join(MAIN_CATEGORIES)}')\n",
    "          AND ay.assessment_year IN ('{\"','\".join(ASSESSMENT_YEAR)}')\n",
    "          AND currency = '{CURRENCY}'\n",
    "   ) WHERE row_no = 1 \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13c38957-8867-4d44-a849-af0113e3ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.createOrReplaceTempView(\"FILTERED_CAMPAIGNS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac3545ad-970d-4923-a390-974fa4dcd56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/.cache/pypoetry/virtualenvs/dataqualityandvalidation-N5_6aXR_-py3.8/lib/python3.8/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "               id                                               name currency  \\\n0           50419     Graphic design tools for award-winning designs      USD   \n1          303187                          Paleocene #1 (Comic Book)      USD   \n2          821031                  T21 Blaster : A Star Wars project      USD   \n3         1430657                           Hobnobbin' with Slim Man      USD   \n4         1694298           Vampire Scarlett Playing Cards Deck-NSFW      USD   \n...           ...                                                ...      ...   \n17851  2146763433  SAYER - Season 4 of the Narrative Science Fict...      USD   \n17852  2146764201                                                Row      USD   \n17853  2146831280  KicoBox- A Robot like RubikÃ¢ÂÂs Cube to Tea...      USD   \n17854  2147144291                               Jubilee Espresso Rub      USD   \n17855  2147345648  Symbolicons Pro: Your friendly, versatile, go-...      USD   \n\n      main_category          launched_at             deadline goal_usd  \\\n0            design  2018-05-02 18:22:31  2018-06-01 18:22:31   1250.0   \n1            comics  2017-05-08 07:04:46  2017-06-07 07:04:46    400.0   \n2               art  2017-08-30 18:10:39  2017-10-29 18:10:39    300.0   \n3        journalism  2017-05-11 00:02:44  2017-06-10 00:02:44   3000.0   \n4             games  2018-06-05 15:15:18  2018-06-12 15:15:18     99.0   \n...             ...                  ...                  ...      ...   \n17851    publishing  2017-01-10 21:56:15  2017-02-09 21:56:15   4000.0   \n17852  film & video  2016-07-11 19:52:31  2016-08-10 19:52:31   5750.0   \n17853    technology  2018-03-01 15:12:02  2018-03-27 14:12:02  10000.0   \n17854          food  2017-06-15 18:19:18  2017-06-30 18:19:18   1000.0   \n17855    technology  2017-07-26 13:13:04  2017-08-18 18:00:00  12000.0   \n\n      country usd_pledged      status assessment_year  \n0          US      2985.0  successful            2018  \n1          US     1661.52  successful            2017  \n2          US       301.0  successful            2018  \n3          US      6813.0  successful            2017  \n4          US       653.0  successful            2018  \n...       ...         ...         ...             ...  \n17851      US     17048.0  successful            2017  \n17852      US     5862.42  successful            2017  \n17853      US     12914.0  successful            2018  \n17854      US      1642.0  successful            2017  \n17855      US     70751.0  successful            2018  \n\n[17856 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>currency</th>\n      <th>main_category</th>\n      <th>launched_at</th>\n      <th>deadline</th>\n      <th>goal_usd</th>\n      <th>country</th>\n      <th>usd_pledged</th>\n      <th>status</th>\n      <th>assessment_year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50419</td>\n      <td>Graphic design tools for award-winning designs</td>\n      <td>USD</td>\n      <td>design</td>\n      <td>2018-05-02 18:22:31</td>\n      <td>2018-06-01 18:22:31</td>\n      <td>1250.0</td>\n      <td>US</td>\n      <td>2985.0</td>\n      <td>successful</td>\n      <td>2018</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>303187</td>\n      <td>Paleocene #1 (Comic Book)</td>\n      <td>USD</td>\n      <td>comics</td>\n      <td>2017-05-08 07:04:46</td>\n      <td>2017-06-07 07:04:46</td>\n      <td>400.0</td>\n      <td>US</td>\n      <td>1661.52</td>\n      <td>successful</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>821031</td>\n      <td>T21 Blaster : A Star Wars project</td>\n      <td>USD</td>\n      <td>art</td>\n      <td>2017-08-30 18:10:39</td>\n      <td>2017-10-29 18:10:39</td>\n      <td>300.0</td>\n      <td>US</td>\n      <td>301.0</td>\n      <td>successful</td>\n      <td>2018</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1430657</td>\n      <td>Hobnobbin' with Slim Man</td>\n      <td>USD</td>\n      <td>journalism</td>\n      <td>2017-05-11 00:02:44</td>\n      <td>2017-06-10 00:02:44</td>\n      <td>3000.0</td>\n      <td>US</td>\n      <td>6813.0</td>\n      <td>successful</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1694298</td>\n      <td>Vampire Scarlett Playing Cards Deck-NSFW</td>\n      <td>USD</td>\n      <td>games</td>\n      <td>2018-06-05 15:15:18</td>\n      <td>2018-06-12 15:15:18</td>\n      <td>99.0</td>\n      <td>US</td>\n      <td>653.0</td>\n      <td>successful</td>\n      <td>2018</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17851</th>\n      <td>2146763433</td>\n      <td>SAYER - Season 4 of the Narrative Science Fict...</td>\n      <td>USD</td>\n      <td>publishing</td>\n      <td>2017-01-10 21:56:15</td>\n      <td>2017-02-09 21:56:15</td>\n      <td>4000.0</td>\n      <td>US</td>\n      <td>17048.0</td>\n      <td>successful</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>17852</th>\n      <td>2146764201</td>\n      <td>Row</td>\n      <td>USD</td>\n      <td>film &amp; video</td>\n      <td>2016-07-11 19:52:31</td>\n      <td>2016-08-10 19:52:31</td>\n      <td>5750.0</td>\n      <td>US</td>\n      <td>5862.42</td>\n      <td>successful</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>17853</th>\n      <td>2146831280</td>\n      <td>KicoBox- A Robot like RubikÃ¢ÂÂs Cube to Tea...</td>\n      <td>USD</td>\n      <td>technology</td>\n      <td>2018-03-01 15:12:02</td>\n      <td>2018-03-27 14:12:02</td>\n      <td>10000.0</td>\n      <td>US</td>\n      <td>12914.0</td>\n      <td>successful</td>\n      <td>2018</td>\n    </tr>\n    <tr>\n      <th>17854</th>\n      <td>2147144291</td>\n      <td>Jubilee Espresso Rub</td>\n      <td>USD</td>\n      <td>food</td>\n      <td>2017-06-15 18:19:18</td>\n      <td>2017-06-30 18:19:18</td>\n      <td>1000.0</td>\n      <td>US</td>\n      <td>1642.0</td>\n      <td>successful</td>\n      <td>2017</td>\n    </tr>\n    <tr>\n      <th>17855</th>\n      <td>2147345648</td>\n      <td>Symbolicons Pro: Your friendly, versatile, go-...</td>\n      <td>USD</td>\n      <td>technology</td>\n      <td>2017-07-26 13:13:04</td>\n      <td>2017-08-18 18:00:00</td>\n      <td>12000.0</td>\n      <td>US</td>\n      <td>70751.0</td>\n      <td>successful</td>\n      <td>2018</td>\n    </tr>\n  </tbody>\n</table>\n<p>17856 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc82351-dcb4-45cd-b59f-ba5d012a6207",
   "metadata": {},
   "source": [
    "### Valid filtered Data\n",
    "\n",
    "#### Create a SparkDFDataset instance of filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3d7f3ce-2c05-4914-aad9-c0b1e0fff86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_df = SparkDFDataset(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b561fa-a10a-4b44-8f45-37ef77bc2523",
   "metadata": {},
   "source": [
    "#### 2.1 Test 1: Check if filter_df main_category within scope\n",
    "\n",
    "Here we use expect_column_values_to_be_in_set(colName, expected_value_list)\n",
    "\n",
    "The result will contain values which are not in the list\n",
    "\n",
    "```text\n",
    "\"result\": {\n",
    "    \"element_count\": 17856,\n",
    "    \"missing_count\": 0,\n",
    "    \"missing_percent\": 0.0,\n",
    "    \"unexpected_count\": 0,\n",
    "    \"unexpected_percent\": 0.0,\n",
    "    \"unexpected_percent_total\": 0.0,\n",
    "    \"unexpected_percent_nonmissing\": 0.0,\n",
    "    \"partial_unexpected_list\": []\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result content :\n",
      " {\n",
      "  \"expectation_config\": {\n",
      "    \"meta\": {},\n",
      "    \"expectation_type\": \"expect_column_values_to_be_in_set\",\n",
      "    \"kwargs\": {\n",
      "      \"column\": \"main_category\",\n",
      "      \"value_set\": [\n",
      "        \"art\",\n",
      "        \"publishing\",\n",
      "        \"film & video\",\n",
      "        \"technology\",\n",
      "        \"journalism\",\n",
      "        \"food\",\n",
      "        \"dance\",\n",
      "        \"photography\",\n",
      "        \"games\",\n",
      "        \"crafts\",\n",
      "        \"music\",\n",
      "        \"comics\",\n",
      "        \"theater\",\n",
      "        \"design\"\n",
      "      ],\n",
      "      \"result_format\": \"BASIC\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": {\n",
      "    \"element_count\": 17856,\n",
      "    \"missing_count\": 0,\n",
      "    \"missing_percent\": 0.0,\n",
      "    \"unexpected_count\": 0,\n",
      "    \"unexpected_percent\": 0.0,\n",
      "    \"unexpected_percent_total\": 0.0,\n",
      "    \"unexpected_percent_nonmissing\": 0.0,\n",
      "    \"partial_unexpected_list\": []\n",
      "  },\n",
      "  \"meta\": {},\n",
      "  \"success\": true,\n",
      "  \"exception_info\": {\n",
      "    \"raised_exception\": false,\n",
      "    \"exception_traceback\": null,\n",
      "    \"exception_message\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = filtered_test_df.expect_column_values_to_be_in_set(\"main_category\", MAIN_CATEGORIES)\n",
    "print(f\"result content :\\n {result}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d54aabb-ce9d-4ad0-b7cf-3c398460a40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories are within scope: PASSED\n"
     ]
    }
   ],
   "source": [
    "result = filtered_test_df.expect_column_values_to_be_in_set(\"main_category\", MAIN_CATEGORIES)\n",
    "print(f\"\"\"Categories are within scope: {'PASSED' if result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d1ae9-7b21-4a39-8f7c-f2aaa8470afa",
   "metadata": {},
   "source": [
    "#### 2.2 Test 2: Check if country is equal to \"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f033b595-ae76-40be-b702-4f0837f514fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:============================================>         (165 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All campaigns are done in the country of USA: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_result = filtered_test_df.expect_column_values_to_be_in_set(\"country\", [\"US\"])\n",
    "print(f\"\"\"All campaigns are done in the country of USA: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861de98-649f-4fae-8dac-13e8fef977f7",
   "metadata": {},
   "source": [
    "#### 2.3 Test 3: Check if status = 'success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76329016-951d-4637-992c-f6ad03b7e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All campaigns are successful: PASSED\n"
     ]
    }
   ],
   "source": [
    "test_result = filtered_test_df.expect_column_values_to_be_in_set(\"status\", [\"successful\"])\n",
    "print(f\"\"\"All campaigns are successful: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f806c-d6d5-4c4b-94bc-f5f85cde0b5e",
   "metadata": {},
   "source": [
    "#### 2.4 Test 4: Check if currency = 'USD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e186b06b-173a-4527-b39e-62b9707fbe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All campaigns are successful: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_result = filtered_test_df.expect_column_values_to_be_in_set(\"currency\", [\"USD\"])\n",
    "print(f\"\"\"All campaigns are successful: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7ca3e-5601-4686-a78c-e8482c0c9b07",
   "metadata": {},
   "source": [
    "#### 2.5 Test 5: Check if mandatory columns are not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3df4abfa-ef33-4e10-b7d1-2e4d726220cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column id are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column currency are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column main_category are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column launched_at are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column deadline are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column country are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column status are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 151:===================================================> (195 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items in column usd_pledged are not null: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for column in MANDATORY_COLUMNS:\n",
    "    try:\n",
    "        test_result = filtered_test_df.expect_column_values_to_not_be_null(column)\n",
    "        assert test_result.success, f\"Uh oh! {test_result.result['unexpected_count']} of {test_result.result['element_count']} items in column {column} are null: FAILED\"\n",
    "        print(f\"All items in column {column} are not null: PASSED\")\n",
    "    except AssertionError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2c348-5774-4cf5-8549-261ecfb4ab27",
   "metadata": {},
   "source": [
    "#### 2.6 Test 6: Check if id is unique in each assessment year\n",
    "\n",
    "The **expect_compound_columns_to_be_unique** take a list of columns and check if the value combination of the given column are unique. We can use it to detect duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba39e78b-3731-4c9f-81b2-343b1dbb682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 161:====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id column is unique for each assessment year: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_result = filtered_test_df.expect_compound_columns_to_be_unique([\"id\", \"assessment_year\"])\n",
    "print(f\"\"\"id column is unique for each assessment year: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0a1dd-1e84-4afa-828f-4077ddf2200a",
   "metadata": {},
   "source": [
    "#### 2.7 Test 7: Check if launched_at is a valid datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41dcb343-2848-40a7-9df5-f81afbe89733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "'launched_at column values are compliant to datetime format: PASSED'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result =  filtered_test_df.expect_column_values_to_match_strftime_format('launched_at','%Y-%m-%d %H:%M:%S')\n",
    "f\"\"\"launched_at column values are compliant to datetime format: {'PASSED' if test_result.success else 'FAILED'}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b87b8d-131d-4611-96a9-d7d8651ed22a",
   "metadata": {},
   "source": [
    "#### 2.8 Test 8: Check if deadline is a valid datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66f23789-5e71-4330-b22c-85361243cc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "'deadline column values are compliant to datetime format: PASSED'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result =  filtered_test_df.expect_column_values_to_match_strftime_format('deadline','%Y-%m-%d %H:%M:%S')\n",
    "f\"\"\"deadline column values are compliant to datetime format: {'PASSED' if test_result.success else 'FAILED'}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d71ab-0fa9-433a-8893-59bb9513c53c",
   "metadata": {},
   "source": [
    "## 3. Standardise Data\n",
    "\n",
    "### Business Rules\n",
    "- Reduce metric categories to 6 final categories\n",
    "- Define Pledge Categorise based on Total Pledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c199a42d-43de-406a-9803-46ff9272d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_CATEGORIES = [\n",
    "    'art, crafts, photography & design',\n",
    "    'dance & theater',\n",
    "    'music, film & video',\n",
    "    'comics, publishing & journalism',\n",
    "    'games & technology',\n",
    "    'food'  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6d8e0b29-31aa-4593-adce-17b25ae62a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLEDGE_CATEGORIES = [\n",
    "    '100 Thousand and under',\n",
    "    'Between 100 Thousand and 500 Thousand',\n",
    "    'Between 500 Thousand and 1 Million',\n",
    "    'Between 1 Million and 5 Million',\n",
    "    '5 Million and over'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f96056-977b-4748-982d-ddff310a3c29",
   "metadata": {},
   "source": [
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13f4c1-d6c9-4242-8628-5045adba8e0c",
   "metadata": {},
   "source": [
    "##### Apply Transformation and create a view named STANDARDISED_CAMPAIGNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e014bda2-dbd2-4e0d-ab57-d3a533cce6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardised_df = spark.sql(f\"\"\"\n",
    "    SELECT t.*,\n",
    "         CASE \n",
    "             WHEN main_category IN ('art','crafts','photography','design') THEN 'art, crafts, photography & design'\n",
    "              WHEN main_category IN ('dance','theater') THEN 'dance & theater'\n",
    "              WHEN main_category IN ('music','film & video') THEN 'music, film & video'\n",
    "              WHEN main_category IN ('comics','publishing','journalism') THEN 'comics, publishing & journalism'\n",
    "              WHEN main_category IN ('games', 'technology') THEN 'games & technology'\n",
    "              WHEN main_category IN ('food') THEN 'food'                  \n",
    "             END metric_category,\n",
    "         CASE WHEN usd_pledged <= 100000 THEN '100 Thousand and under'\n",
    "              WHEN usd_pledged > 100000 AND usd_pledged < 500000 THEN 'Between 100 Thousand and 500 Thousand'\n",
    "              WHEN usd_pledged > 500000 AND usd_pledged < 1000000 THEN 'Between 500 Thousand and 1 Million'\n",
    "              WHEN usd_pledged > 1000000 AND usd_pledged < 5000000 THEN 'Between 1 Million and 5 Million'\n",
    "           ELSE '5 Million and over'\n",
    "           END pledge_category    \n",
    "    FROM FILTERED_CAMPAIGNS t       \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce783c3c-debd-4072-8a54-89bef088118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardised_df.createOrReplaceTempView(\"STANDARDISED_CAMPAIGNS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab57f5-2737-4e8e-a009-9fbbe4bc12ae",
   "metadata": {},
   "source": [
    "### Valid Standardised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97db58f2-5e76-40a3-8fea-b7bc453e95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkDFDataset instance of standardised_df\n",
    "standardised_test_df = SparkDFDataset(standardised_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1373aca-7699-4536-a000-97504f8301ba",
   "metadata": {},
   "source": [
    "#### 3.1 Test 1: Check if metric_category is within scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a7400b0-885c-41be-af0b-01da1df7ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:===================================================> (194 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories are within scope: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_result = standardised_test_df.expect_column_values_to_be_in_set(\"metric_category\", METRIC_CATEGORIES)\n",
    "\n",
    "print(f\"\"\"Categories are within scope: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1bb616-e0b1-45c5-9c94-2af44b7b7bfd",
   "metadata": {},
   "source": [
    "#### 3.2 Test 2: Check if campaign population is equal to the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc8236e5-fbba-4555-8433-fed71eaccdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 177:==================================================>  (191 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories are within scope: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_result = standardised_test_df.expect_column_values_to_be_in_set(\"pledge_category\", PLEDGE_CATEGORIES)\n",
    "\n",
    "print(f\"\"\"Categories are within scope: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c2ef5-e5ad-4939-99ae-62f95567e23c",
   "metadata": {},
   "source": [
    "#### 3.3 Test 3: Check if campaign population is equal to the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e1ccafd-9999-4f37-aa67-01e5184e7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count of standardised_df (17856) is equal to total row count of filtered_df (17856): PASSED\n"
     ]
    }
   ],
   "source": [
    "filtered_total_rows = filtered_test_df.get_row_count()\n",
    "test_result = standardised_test_df.expect_table_row_count_to_equal(filtered_total_rows)\n",
    "\n",
    "print(f\"\"\"Total row count of standardised_df ({test_result.result['observed_value']}) \\\n",
    "is equal to total row count of filtered_df ({filtered_total_rows}): \\\n",
    "{'PASSED' if test_result.result['observed_value'] == filtered_total_rows else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4c347-c605-4b9c-b3d7-5d749774d2b7",
   "metadata": {},
   "source": [
    "## 4. Generate Metrics\n",
    "\n",
    "### Business Rules\n",
    "\n",
    "- Metric #1: Count Number of successful campaigns for each metric category and pledge category per assessment year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d47636-cbc0-422e-9bdb-57a9c5db6bff",
   "metadata": {},
   "source": [
    "### Generate SQL to produce Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3f73fee-f266-4d3d-b661-dfcf52e5f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_campaigns_df = spark.sql(f\"\"\"\n",
    "    SELECT assessment_year,\n",
    "           metric_category,\n",
    "           pledge_category,\n",
    "           count(id) total_successful_campaigns\n",
    "    FROM STANDARDISED_CAMPAIGNS\n",
    "    GROUP BY assessment_year,\n",
    "            metric_category,\n",
    "            pledge_category\n",
    "    ORDER BY assessment_year,\n",
    "            metric_category,\n",
    "            pledge_category\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef0f15c8-6ca0-4507-a904-ecda803b07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_campaigns_df.createOrReplaceTempView(\"SUCCESSFUL_CAMPAIGNS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee9fd53-88e8-4354-8a20-c01accce9eed",
   "metadata": {},
   "source": [
    "## Unit Tests on Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04810607-b8e4-481e-ae0b-c1ceac19a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkDFDataset instance of successful_campaigns_df\n",
    "successful_campaigns_df_test_df = SparkDFDataset(successful_campaigns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa39409-8918-4c6c-943a-97511abfcb8a",
   "metadata": {},
   "source": [
    "#### 4.1 Test 1: Check if metric_category and pledge_category pair is unique for each assessment year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db231015-fbfa-4d6e-b6a7-33e4e4e7aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric_category column is unique for each assessment year: PASSED\n"
     ]
    }
   ],
   "source": [
    "test_result = successful_campaigns_df_test_df.expect_compound_columns_to_be_unique([\"assessment_year\",\"metric_category\",\"pledge_category\"])\n",
    "\n",
    "print(f\"\"\"metric_category column is unique for each assessment year: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b3944-c623-4604-b32e-2a689249cb31",
   "metadata": {},
   "source": [
    "#### 4.2 Test 2: Check if sum total of campaigns in metrics dataset is equal to total campaigns in standardised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8afa33a-3f43-42e6-a639-e588eaf88c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 229:=======================================>               (27 + 2) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of campaigns in metrics dataset (17856) is equal to total rows in standardised dataset (17856): PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "standardised_total_rows = standardised_test_df.get_row_count()\n",
    "test_result = successful_campaigns_df_test_df.expect_column_sum_to_be_between('total_successful_campaigns', \n",
    "                                                                              standardised_total_rows, standardised_total_rows)\n",
    "\n",
    "print(f\"\"\"Total sum of campaigns in metrics dataset ({test_result.result['observed_value']}) \\\n",
    "is equal to total rows in standardised dataset ({standardised_total_rows}): \\\n",
    "{'PASSED' if test_result.result['observed_value'] ==  standardised_total_rows else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47dff0e-d923-4963-aeb6-36388b6bc22f",
   "metadata": {},
   "source": [
    "## Integration Tests\n",
    "\n",
    "When we call an expectation function on a great expectation spark dataframe, the expectation is registered to the dataframe. When you call dataframe.validate() against the Great Expectations dataset (SparkDFDataSet). All registered expectation will be evaluated, and you will get an overall result.\n",
    "\n",
    "\n",
    "\n",
    "check below example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Raw dataset validations: {\n",
      "  \"meta\": {\n",
      "    \"great_expectations_version\": \"0.15.24\",\n",
      "    \"expectation_suite_name\": \"default\",\n",
      "    \"run_id\": {\n",
      "      \"run_time\": \"2023-01-12T11:14:36.408979+00:00\",\n",
      "      \"run_name\": null\n",
      "    },\n",
      "    \"batch_kwargs\": {\n",
      "      \"ge_batch_id\": \"a06ca0fe-925e-11ed-a5da-f5d72802541e\"\n",
      "    },\n",
      "    \"batch_markers\": {},\n",
      "    \"batch_parameters\": {},\n",
      "    \"validation_time\": \"20230112T111436.408886Z\",\n",
      "    \"expectation_suite_meta\": {\n",
      "      \"great_expectations_version\": \"0.15.24\"\n",
      "    }\n",
      "  },\n",
      "  \"success\": false,\n",
      "  \"statistics\": {\n",
      "    \"evaluated_expectations\": 12,\n",
      "    \"successful_expectations\": 2,\n",
      "    \"unsuccessful_expectations\": 10,\n",
      "    \"success_percent\": 16.666666666666664\n",
      "  },\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"id\",\n",
      "          \"type_\": \"StringType\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"observed_value\": \"IntegerType\"\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"id\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 0,\n",
      "        \"unexpected_percent\": 0.0,\n",
      "        \"unexpected_percent_total\": 0.0,\n",
      "        \"partial_unexpected_list\": []\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": true,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_be_unique\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"id\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"missing_count\": 0,\n",
      "        \"missing_percent\": 0.0,\n",
      "        \"unexpected_count\": 48162,\n",
      "        \"unexpected_percent\": 25.01298377547417,\n",
      "        \"unexpected_percent_total\": 25.01298377547417,\n",
      "        \"unexpected_percent_nonmissing\": 25.01298377547417,\n",
      "        \"partial_unexpected_list\": [\n",
      "          39036,\n",
      "          39036,\n",
      "          39235,\n",
      "          39235,\n",
      "          50419,\n",
      "          50419,\n",
      "          188790,\n",
      "          188790,\n",
      "          342881,\n",
      "          342881,\n",
      "          358771,\n",
      "          358771,\n",
      "          377517,\n",
      "          377517,\n",
      "          390870,\n",
      "          390870,\n",
      "          442565,\n",
      "          442565,\n",
      "          538372,\n",
      "          538372\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"main_category\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 1,\n",
      "        \"unexpected_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_percent_total\": 0.0005193510189666992,\n",
      "        \"partial_unexpected_list\": [\n",
      "          null\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"currency\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 0,\n",
      "        \"unexpected_percent\": 0.0,\n",
      "        \"unexpected_percent_total\": 0.0,\n",
      "        \"partial_unexpected_list\": []\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": true,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"launched_at\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 1,\n",
      "        \"unexpected_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_percent_total\": 0.0005193510189666992,\n",
      "        \"partial_unexpected_list\": [\n",
      "          null\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_match_strftime_format\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"launched_at\",\n",
      "          \"strftime_format\": \"%Y-%m-%d %H:%M:%S\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"missing_count\": 1,\n",
      "        \"missing_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_count\": 562,\n",
      "        \"unexpected_percent\": 0.2918767885243603,\n",
      "        \"unexpected_percent_total\": 0.2918752726592849,\n",
      "        \"unexpected_percent_nonmissing\": 0.2918767885243603,\n",
      "        \"partial_unexpected_list\": [\n",
      "          \"Hip-Hop\",\n",
      "          \"Rock\",\n",
      "          \"Webseries\",\n",
      "          \"Musical\",\n",
      "          \"Kids\",\n",
      "          \"film & video\",\n",
      "          \"Painting\",\n",
      "          \"Webseries\",\n",
      "          \"Comedy\",\n",
      "          \"World Music\",\n",
      "          \"Children's Books\",\n",
      "          \"Indie Rock\",\n",
      "          \"Documentary\",\n",
      "          \"Cookbooks\",\n",
      "          \"Country & Folk\",\n",
      "          \"Hip-Hop\",\n",
      "          \"Jazz\",\n",
      "          \"Shorts\",\n",
      "          \"Classical Music\",\n",
      "          \"Art Books\"\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"deadline\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 1,\n",
      "        \"unexpected_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_percent_total\": 0.0005193510189666992,\n",
      "        \"partial_unexpected_list\": [\n",
      "          null\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_match_strftime_format\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"deadline\",\n",
      "          \"strftime_format\": \"%Y-%m-%d %H:%M:%S\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"missing_count\": 1,\n",
      "        \"missing_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_count\": 86,\n",
      "        \"unexpected_percent\": 0.04466441959625442,\n",
      "        \"unexpected_percent_total\": 0.04466418763113613,\n",
      "        \"unexpected_percent_nonmissing\": 0.04466441959625442,\n",
      "        \"partial_unexpected_list\": [\n",
      "          \"Documentary\",\n",
      "          \"Metal\",\n",
      "          \"Country & Folk\",\n",
      "          \"Indie Rock\",\n",
      "          \"Performance Art\",\n",
      "          \"Performance Art\",\n",
      "          \"Country & Folk\",\n",
      "          \"Jazz\",\n",
      "          \"Experimental\",\n",
      "          \"Webseries\",\n",
      "          \"Digital Art\",\n",
      "          \"Pop\",\n",
      "          \"Musical\",\n",
      "          \"Children's Books\",\n",
      "          \"Art Books\",\n",
      "          \"Children's Books\",\n",
      "          \"EUR\",\n",
      "          \"Illustration\",\n",
      "          \"Plays\",\n",
      "          \"Fiction\"\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"country\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 1,\n",
      "        \"unexpected_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_percent_total\": 0.0005193510189666992,\n",
      "        \"partial_unexpected_list\": [\n",
      "          null\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"status\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 1,\n",
      "        \"unexpected_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_percent_total\": 0.0005193510189666992,\n",
      "        \"partial_unexpected_list\": [\n",
      "          null\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"expectation_config\": {\n",
      "        \"meta\": {},\n",
      "        \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
      "        \"kwargs\": {\n",
      "          \"column\": \"usd_pledged\",\n",
      "          \"result_format\": \"BASIC\"\n",
      "        }\n",
      "      },\n",
      "      \"result\": {\n",
      "        \"element_count\": 192548,\n",
      "        \"unexpected_count\": 1,\n",
      "        \"unexpected_percent\": 0.0005193510189666992,\n",
      "        \"unexpected_percent_total\": 0.0005193510189666992,\n",
      "        \"partial_unexpected_list\": [\n",
      "          null\n",
      "        ]\n",
      "      },\n",
      "      \"meta\": {},\n",
      "      \"success\": false,\n",
      "      \"exception_info\": {\n",
      "        \"raised_exception\": false,\n",
      "        \"exception_message\": null,\n",
      "        \"exception_traceback\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"evaluation_parameters\": {}\n",
      "} \n"
     ]
    }
   ],
   "source": [
    "raw_test_df_validation = raw_test_df.validate()\n",
    "print(f\"\"\"1. Raw dataset validations: {raw_test_df_validation} \"\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c80b475d-a587-414a-ade4-efe2336731e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Raw dataset validations: False; 52.63157894736842 successful\n",
      "2. Filtered dataset validations: True; 100.0 successful\n",
      "3. Standardised dataset validations: True; 100.0 successful\n",
      "4. Metrics dataset validations: True; 100.0 successful\n"
     ]
    }
   ],
   "source": [
    "raw_test_df_validation = raw_test_df.validate()\n",
    "print(f\"\"\"1. Raw dataset validations: {raw_test_df_validation.success}; {raw_test_df_validation.statistics['success_percent']} successful\"\"\")\n",
    "filtered_test_df_validation = filtered_test_df.validate()\n",
    "print(f\"\"\"2. Filtered dataset validations: {filtered_test_df_validation.success}; {filtered_test_df_validation.statistics['success_percent']} successful\"\"\")\n",
    "standardised_test_df_validation = standardised_test_df.validate()\n",
    "print(f\"\"\"3. Standardised dataset validations: {standardised_test_df_validation.success}; {standardised_test_df_validation.statistics['success_percent']} successful\"\"\")\n",
    "successful_campaigns_df_test_df_validation = successful_campaigns_df_test_df.validate()\n",
    "print(f\"\"\"4. Metrics dataset validations: {successful_campaigns_df_test_df_validation.success}; {successful_campaigns_df_test_df_validation.statistics['success_percent']} successful\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c6ae2-9d3b-421b-87f3-01cb89f50f12",
   "metadata": {},
   "source": [
    "## 5. Custom expectation functions\n",
    "\n",
    "If you can't find an expectation function that fits your needs,you can define your own expectation functions. GE provides several methods for building and deploying custom expectations.\n",
    "\n",
    "In this tutorial, we only focus on the `expectation decorators`. For more information, you can read this [article](https://docs.greatexpectations.io/docs/guides/expectations/creating_custom_expectations/overview).\n",
    "\n",
    "There are two major decorators:\n",
    "- **column_map_expectations**: which apply their condition to each value in a column independently of other values\n",
    "- **column_aggregate_expectations**: which apply their condition to an aggregate value or values from the column\n",
    "\n",
    "Major steps on implementing a custom expectation function:\n",
    "\n",
    "1. Create a `subclass` from the dataset class of your choice\n",
    "\n",
    "2. Define custom functions containing your business logic\n",
    "\n",
    "3. Use the `column_map_expectation` and `column_aggregate_expectation` decorators to turn them into full Expectations. Note that each dataset class implements its own versions of @column_map_expectation and @column_aggregate_expectation, so you should consult the documentation of each class to ensure you are returning the correct information to the decorator.\n",
    "\n",
    "> You better follow the expectation naming convention.\n",
    "\n",
    "\n",
    "### Pandas example\n",
    "\n",
    "```python\n",
    "from great_expectations.dataset import PandasDataset, MetaPandasDataset\n",
    "\n",
    "class CustomPandasDataset(PandasDataset):\n",
    "\n",
    "    _data_asset_type = \"CustomPandasDataset\"\n",
    "\n",
    "    @MetaPandasDataset.column_map_expectation\n",
    "    def expect_column_values_to_equal_2(self, column):\n",
    "        return column.map(lambda x: x==2)\n",
    "\n",
    "    @MetaPandasDataset.column_aggregate_expectation\n",
    "    def expect_column_mode_to_equal_0(self, column):\n",
    "        mode = self[column].mode[0]\n",
    "        return {\n",
    "            \"success\" : mode == 0,\n",
    "            \"result\": {\n",
    "                \"observed_value\": mode,\n",
    "            }\n",
    "        }\n",
    "\n",
    "```\n",
    "\n",
    "### SqlAlchemy example\n",
    "\n",
    "Below sqlAlchemy\n",
    "```python\n",
    "import sqlalchemy as sa\n",
    "from great_expectations.data_asset import DataAsset\n",
    "from great_expectations.dataset import SqlAlchemyDataset\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "\n",
    "if sys.version_info.major >= 3 and sys.version_info.minor >= 5:\n",
    "    from math import gcd\n",
    "else:\n",
    "    from fractions import gcd\n",
    "\n",
    "class CustomSqlAlchemyDataset(SqlAlchemyDataset):\n",
    "\n",
    "    _data_asset_type = \"CustomSqlAlchemyDataset\"\n",
    "\n",
    "    @DataAsset.expectation([\"column_A\", \"column_B\", \"p_value\", \"mode\"])\n",
    "    def expect_column_pair_histogram_ks_2samp_test_p_value_to_be_greater_than(\n",
    "            self,\n",
    "            column_A,\n",
    "            column_B,\n",
    "            p_value=0.05,\n",
    "            mode='auto'\n",
    "    ):\n",
    "        \"\"\"Execute the two sample KS test on two columns of data that are expected to be **histograms** with\n",
    "        aligned values/points on the CDF. .\"\"\"\n",
    "        LARGE_N = 10000  # 'auto' will attempt to be exact if n1,n2 <= LARGE_N\n",
    "\n",
    "        # We will assume that these are already HISTOGRAMS created as a check_dataset\n",
    "        # either of binned values or of (ordered) value counts\n",
    "        rows = sa.select([\n",
    "            sa.column(column_A).label(\"col_A_counts\"),\n",
    "            sa.column(column_B).label(\"col_B_counts\")\n",
    "        ]).select_from(self._table).fetchall()\n",
    "\n",
    "        cols = [col for col in zip(*rows)]\n",
    "        cdf1 = np.array(cols[0])\n",
    "        cdf2 = np.array(cols[1])\n",
    "        n1 = cdf1.sum()\n",
    "        n2 = cdf2.sum()\n",
    "        cdf1 = cdf1 / n1\n",
    "        cdf2 = cdf2 / n2\n",
    "\n",
    "        # This code is taken verbatim from scipy implementation,\n",
    "        # skipping the searchsorted (using sqlalchemy check asset as a view)\n",
    "        # https://github.com/scipy/scipy/blob/v1.3.1/scipy/stats/stats.py#L5385-L5573\n",
    "        cddiffs = cdf1 - cdf2\n",
    "        minS = -np.min(cddiffs)\n",
    "        maxS = np.max(cddiffs)\n",
    "        alt2Dvalue = {'less': minS, 'greater': maxS, 'two-sided': max(minS, maxS)}\n",
    "        d = alt2Dvalue[alternative]\n",
    "        g = gcd(n1, n2)\n",
    "        n1g = n1 // g\n",
    "        n2g = n2 // g\n",
    "        prob = -np.inf\n",
    "        original_mode = mode\n",
    "        if mode == 'auto':\n",
    "            if max(n1, n2) <= LARGE_N:\n",
    "                mode = 'exact'\n",
    "            else:\n",
    "                mode = 'asymp'\n",
    "        elif mode == 'exact':\n",
    "            # If lcm(n1, n2) is too big, switch from exact to asymp\n",
    "            if n1g >= np.iinfo(np.int).max / n2g:\n",
    "                mode = 'asymp'\n",
    "                warnings.warn(\n",
    "                    \"Exact ks_2samp calculation not possible with samples sizes \"\n",
    "                    \"%d and %d. Switching to 'asymp' \" % (n1, n2), RuntimeWarning)\n",
    "\n",
    "        saw_fp_error = False\n",
    "        if mode == 'exact':\n",
    "            lcm = (n1 // g) * n2\n",
    "            h = int(np.round(d * lcm))\n",
    "            d = h * 1.0 / lcm\n",
    "            if h == 0:\n",
    "                prob = 1.0\n",
    "            else:\n",
    "                try:\n",
    "                    if alternative == 'two-sided':\n",
    "                        if n1 == n2:\n",
    "                            prob = stats._compute_prob_outside_square(n1, h)\n",
    "                        else:\n",
    "                            prob = 1 - stats._compute_prob_inside_method(n1, n2, g, h)\n",
    "                    else:\n",
    "                        if n1 == n2:\n",
    "                            # prob = binom(2n, n-h) / binom(2n, n)\n",
    "                            # Evaluating in that form incurs roundoff errors\n",
    "                            # from special.binom. Instead calculate directly\n",
    "                            prob = 1.0\n",
    "                            for j in range(h):\n",
    "                                prob = (n1 - j) * prob / (n1 + j + 1)\n",
    "                        else:\n",
    "                            num_paths = stats._count_paths_outside_method(n1, n2, g, h)\n",
    "                            bin = special.binom(n1 + n2, n1)\n",
    "                            if not np.isfinite(bin) or not np.isfinite(num_paths) or num_paths > bin:\n",
    "                                raise FloatingPointError()\n",
    "                            prob = num_paths / bin\n",
    "\n",
    "                except FloatingPointError:\n",
    "                    # Switch mode\n",
    "                    mode = 'asymp'\n",
    "                    saw_fp_error = True\n",
    "                    # Can't raise warning here, inside the try\n",
    "                finally:\n",
    "                    if saw_fp_error:\n",
    "                        if original_mode == 'exact':\n",
    "                            warnings.warn(\n",
    "                                \"ks_2samp: Exact calculation overflowed. \"\n",
    "                                \"Switching to mode=%s\" % mode, RuntimeWarning)\n",
    "                    else:\n",
    "                        if prob > 1 or prob < 0:\n",
    "                            mode = 'asymp'\n",
    "                            if original_mode == 'exact':\n",
    "                                warnings.warn(\n",
    "                                    \"ks_2samp: Exact calculation incurred large\"\n",
    "                                    \" rounding error. Switching to mode=%s\" % mode,\n",
    "                                    RuntimeWarning)\n",
    "\n",
    "        if mode == 'asymp':\n",
    "            # The product n1*n2 is large.  Use Smirnov's asymptoptic formula.\n",
    "            if alternative == 'two-sided':\n",
    "                en = np.sqrt(n1 * n2 / (n1 + n2))\n",
    "                # Switch to using kstwo.sf() when it becomes available.\n",
    "                # prob = distributions.kstwo.sf(d, int(np.round(en)))\n",
    "                prob = distributions.kstwobign.sf(en * d)\n",
    "            else:\n",
    "                m, n = max(n1, n2), min(n1, n2)\n",
    "                z = np.sqrt(m*n/(m+n)) * d\n",
    "                # Use Hodges' suggested approximation Eqn 5.3\n",
    "                expt = -2 * z**2 - 2 * z * (m + 2*n)/np.sqrt(m*n*(m+n))/3.0\n",
    "                prob = np.exp(expt)\n",
    "\n",
    "        prob = (0 if prob < 0 else (1 if prob > 1 else prob))\n",
    "\n",
    "        return {\n",
    "            \"success\": prob > p_value,\n",
    "            \"result\": {\n",
    "                \"observed_value\": prob,\n",
    "                \"details\": {\n",
    "                    \"ks_2samp_statistic\": d\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "```\n",
    "\n",
    "### Spark dataset example\n",
    "\n",
    "In below example, we create a `CustomSparkDFDataset` which extends `SparkDFDataset`. we add two custom expectation functions with decorator `@MetaSparkDFDataset.column_aggregate_expectation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e595a49f-7e01-47f2-aad5-e9e5b9f4dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_expectations.dataset import MetaSparkDFDataset\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83452d84-e90d-4547-a286-ac935669a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSparkDFDataset(SparkDFDataset):\n",
    "    _data_asset_type = \"CustomSparkDFDataset\"\n",
    "    \n",
    "    @MetaSparkDFDataset.column_aggregate_expectation\n",
    "    def expect_column_max_to_be_less_than(\n",
    "        self,\n",
    "        column,\n",
    "        value,\n",
    "        strict=False,\n",
    "        parse_strings_as_datetimes=False,\n",
    "        output_strftime_format=None,\n",
    "        result_format=None,\n",
    "        include_config=True,\n",
    "        catch_exceptions=None,\n",
    "        meta=None,\n",
    "    ):\n",
    "        if parse_strings_as_datetimes:\n",
    "            if value:\n",
    "                value = parse(value)\n",
    "\n",
    "        column_max = self.get_column_max(column, parse_strings_as_datetimes)       \n",
    "        if isinstance(column_max, datetime):\n",
    "            try:\n",
    "                value = parse(value)\n",
    "            except (ValueError, TypeError) as e:\n",
    "                pass\n",
    "\n",
    "        success = column_max < value if strict else column_max <= value\n",
    "        \n",
    "        if parse_strings_as_datetimes:\n",
    "            if output_strftime_format:\n",
    "                column_max = datetime.strftime(column_max, output_strftime_format)\n",
    "            else:\n",
    "                column_max = str(column_max)\n",
    "\n",
    "        return {\"success\": success, \"result\": {\"observed_value\": column_max}}  \n",
    "    \n",
    "    @MetaSparkDFDataset.column_aggregate_expectation\n",
    "    def expect_column_min_to_be_more_than(\n",
    "        self,\n",
    "        column,\n",
    "        value,\n",
    "        strict=False,\n",
    "        parse_strings_as_datetimes=False,\n",
    "        output_strftime_format=None,\n",
    "        result_format=None,\n",
    "        include_config=True,\n",
    "        catch_exceptions=None,\n",
    "        meta=None,\n",
    "    ):\n",
    "        if parse_strings_as_datetimes:\n",
    "            if value:\n",
    "                value = parse(value)\n",
    "\n",
    "        column_min = self.get_column_min(column, parse_strings_as_datetimes)       \n",
    "        if isinstance(column_min, datetime):\n",
    "            try:\n",
    "                value = parse(value)\n",
    "            except (ValueError, TypeError) as e:\n",
    "                pass\n",
    "        \n",
    "        success = column_min > value if strict else column_min >= value\n",
    "        \n",
    "        if parse_strings_as_datetimes:\n",
    "            if output_strftime_format:\n",
    "                column_min = datetime.strftime(column_min, output_strftime_format)\n",
    "            else:\n",
    "                column_min = str(column_min)\n",
    "\n",
    "        return {\"success\": success, \"result\": {\"observed_value\": column_min}}      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23769fc1",
   "metadata": {},
   "source": [
    "#### Filtered DF Custom Test 1: Check if earliest launch_at date is not later than the period_end_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f47770f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  assessment_year period_start_dt period_end_dt\n0            2017      2016-07-01    2017-06-30\n1            2018      2017-07-01    2018-06-30",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>assessment_year</th>\n      <th>period_start_dt</th>\n      <th>period_end_dt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017</td>\n      <td>2016-07-01</td>\n      <td>2017-06-30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018</td>\n      <td>2017-07-01</td>\n      <td>2018-06-30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ay_df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the custom expectation\n",
    "To use the custom, we need to first create an instance of the custom dataset instance, then call the custom expectations function of the custom dataset. Below is an example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ca6b9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'2017': <__main__.CustomSparkDFDataset at 0x7f29b858c520>,\n '2018': <__main__.CustomSparkDFDataset at 0x7f29b858c5e0>}"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance of the customSparkDFDataset\n",
    "ge_dataset_by_year = {}\n",
    "for yr in ASSESSMENT_YEAR:\n",
    "    ge_dataset_by_year[yr] = CustomSparkDFDataset(filtered_df.where(f\"assessment_year = {yr}\"))\n",
    "ge_dataset_by_year    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43bb3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_end_dt(yr):\n",
    "    year_filter = ay_df[\"assessment_year\"]==yr\n",
    "    end_dt = ay_df.loc[year_filter].period_end_dt.item()\n",
    "    return datetime.strftime(datetime.strptime(end_dt, \"%Y-%m-%d\") + timedelta(days=1), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0536f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AY 2017 latest launched_at 2017-06-21 20:23:32 < period_end_dt 2017-07-01: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 308:===================================================> (194 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AY 2018 latest launched_at 2018-06-30 23:01:04 < period_end_dt 2018-07-01: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for yr in ASSESSMENT_YEAR:\n",
    "    period_end_dt = get_period_end_dt(yr)   \n",
    "    test_result = ge_dataset_by_year[yr]\\\n",
    "                    .expect_column_max_to_be_less_than(\"launched_at\", \n",
    "                                                       period_end_dt, \n",
    "                                                       parse_strings_as_datetimes=True, \n",
    "                                                       strict=True)\n",
    "    print(f\"\"\"AY {yr} latest launched_at {test_result.result['observed_value']}\"\"\", \n",
    "          f\"\"\"< period_end_dt {period_end_dt}: {'PASSED' if test_result.success else 'FAILED'}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb004d",
   "metadata": {},
   "source": [
    "#### Filtered DF Custom Test 2: Check if earliest campaign deadline is past or falls on the period_start_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c7da5ec-242e-4f83-b64c-c745ce02d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_period_start_dt(yr):\n",
    "    year_filter = ay_df[\"assessment_year\"]==yr\n",
    "    return ay_df.loc[year_filter].period_start_dt.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31e04f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AY 2017 earliest deadline 2016-07-01 00:00:00 >= period_start_dt 2016-07-01: PASSED\n",
      "AY 2018 earliest deadline 2017-07-01 00:00:00 >= period_start_dt 2017-07-01: PASSED\n"
     ]
    }
   ],
   "source": [
    "for yr in ASSESSMENT_YEAR:\n",
    "    period_start_dt = get_period_start_dt(yr)    \n",
    "    test_result = ge_dataset_by_year[yr]\\\n",
    "                    .expect_column_min_to_be_more_than(\"deadline\", \n",
    "                                                       period_start_dt, \n",
    "                                                       parse_strings_as_datetimes=True)\n",
    "    print(f\"\"\"AY {yr} earliest deadline {test_result.result['observed_value']}\"\"\", \n",
    "          f\"\"\">= period_start_dt {period_start_dt}: {'PASSED' if test_result.success else 'FAILED'}\"\"\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005616fb-6f4b-4c61-9127-9b80dc94362c",
   "metadata": {},
   "source": [
    "### Integration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4562edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset for year 2017 validations: True; 100.0 successful\n",
      "Filtered dataset for year 2018 validations: True; 100.0 successful\n"
     ]
    }
   ],
   "source": [
    "for yr, yearly_df in ge_dataset_by_year.items():\n",
    "    yearly_df_validation = yearly_df.validate()\n",
    "    print(f\"\"\"Filtered dataset for year {yr} validations: {yearly_df_validation.success}; {yearly_df_validation.statistics['success_percent']} successful\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
